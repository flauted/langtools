{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informed Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be experimenting with the so-called improved architecture:\n",
    "\n",
    "* numpy improved\n",
    "* numcython improved\n",
    "\n",
    "We'll be\n",
    "* optimizing interfaces for each implementation\n",
    "* experimenting with more cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        for numericalize in [True, False]:\n",
    "            voc = klass(unk=unk)\n",
    "            voc.add(\"hello\")\n",
    "            if numericalize:\n",
    "                voc = num(voc)\n",
    "            assert \"hello\" in voc.string\n",
    "            if unk is not False:\n",
    "                assert unk in voc.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_specials_init(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        for numericalize in [True, False]:\n",
    "            specs = {\"<pad>\", \"<bos>\", \"<eos>\"}\n",
    "            voc = klass(specials=specs, unk=unk)\n",
    "            if numericalize:\n",
    "                voc = num(voc)\n",
    "            for spec in specs:\n",
    "                assert spec in voc.string\n",
    "            if unk is not False:\n",
    "                assert unk in voc.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strip_min_freq(klass, num):\n",
    "    for numericalize in [True, False]:\n",
    "        for unk in [\"UNK\", False]:\n",
    "            all_words = {\"one\", \"two\", \"three\", \"three_again\", \"four\"}\n",
    "            for freq, *words in [\n",
    "                    (0, \"one\", \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (1, \"one\", \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (2, \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (3, \"three\", \"three_again\", \"four\"),\n",
    "                    (4, \"four\")]:\n",
    "                voc = klass(unk=unk)\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three\")\n",
    "                for _ in range(2):\n",
    "                    voc.add(\"two\")\n",
    "                for _ in range(1):\n",
    "                    voc.add(\"one\")\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three_again\")\n",
    "                for _ in range(4):\n",
    "                    voc.add(\"four\")\n",
    "                \n",
    "                if numericalize:\n",
    "                    voc = num(voc)\n",
    "\n",
    "                voc.strip(min_freq=freq)\n",
    "                if unk:\n",
    "                    assert unk in voc.string\n",
    "                for word in words:\n",
    "                    assert word in voc.string\n",
    "                for word in all_words:\n",
    "                    if word not in words:\n",
    "                        try:\n",
    "                            assert word not in voc.string\n",
    "                        except:\n",
    "                            print(f\"case numericalize={numericalize} \"\n",
    "                                  f\"unk={unk} freq={freq} word={word} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strip_by_n(klass, num):\n",
    "    for numericalize in [True, False]:\n",
    "        for unk in [\"UNK\", False]:\n",
    "            all_words = {\"one\", \"two\", \"three\", \"three_again\", \"four\"}\n",
    "            for n_words, *words in [\n",
    "                    (5, \"one\", \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (4, \"four\", \"three\", \"three_again\", \"two\"),\n",
    "                    (3, \"four\", \"three\", \"three_again\"),\n",
    "                    (1, \"four\")]:\n",
    "                voc = klass(unk=unk)\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three\")\n",
    "                for _ in range(2):\n",
    "                    voc.add(\"two\")\n",
    "                for _ in range(1):\n",
    "                    voc.add(\"one\")\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three_again\")\n",
    "                for _ in range(4):\n",
    "                    voc.add(\"four\")\n",
    "\n",
    "                if numericalize:\n",
    "                    voc = num(voc)\n",
    "\n",
    "                voc.strip(n_to_keep=n_words)\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        assert word in voc.string\n",
    "                    except:\n",
    "                        print(f\"case IN numericalize={numericalize} \"\n",
    "                              f\"unk={unk} n_words={n_words} word={word} failed\")\n",
    "                for word in all_words:\n",
    "                    if word not in words:\n",
    "                        try:\n",
    "                            assert word not in voc.string\n",
    "                        except:\n",
    "                            print(\n",
    "                                f\"case NOT_IN numericalize={numericalize} \"\n",
    "                                f\"unk={unk} n_words={n_words} word={word} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_num_identity(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        voc.add(\"hello\")\n",
    "        voc = num(voc)\n",
    "        assert \"hello\" == voc.string[voc.integer[\"hello\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_num_identity_unk(klass, num):\n",
    "    voc = klass(unk=\"UNK\")\n",
    "    voc = num(voc)\n",
    "    assert \"UNK\" == voc.string[voc.integer[\"jambalaya\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out_of_vocab_no_unk(klass, num):\n",
    "    voc = klass(unk=False)\n",
    "    voc = num(voc)\n",
    "    try:\n",
    "        voc.string[voc.integer[\"jambalaya\"]]\n",
    "    except:\n",
    "        pass  # lol, PASS!\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_1d(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [2, 3, 4, 1, 2]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three four\"\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_1d_unk(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [2, 3, 4, 0, 0]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three four UNK UNK\"\n",
    "    ])\n",
    "    voc = klass(unk=\"UNK\")\n",
    "    for _ in range(6):\n",
    "        voc.add(\"one\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    voc.permit_unk(True)\n",
    "    s = voc.sentence(fake_data)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_2d_axis_0(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [[2, 3, 4, 1, 2],\n",
    "         [3, 2, 4, 2, 1],\n",
    "         [2, 3, 0, 0, 0]]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three two\", \"three two three\", \"four four\", \"\", \"two\"\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, axis=0)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_2d_axis_1(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [[2, 3, 4, 1, 2],\n",
    "         [3, 2, 4, 2, 1],\n",
    "         [2, 3, 0, 0, 0]]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three four\", \"three two four two\", \"two three\"\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, axis=1)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_3d_axis_2(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [[[2, 3, 4, 1, 2],\n",
    "          [3, 2, 4, 2, 1],\n",
    "          [2, 3, 0, 0, 0]],\n",
    "         [[1, 0, 0, 0, 0],\n",
    "          [3, 4, 2, 3, 4],\n",
    "          [4, 4, 3, 3, 0]]]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        [\"two three four\", \"three two four two\", \"two three\"],\n",
    "        [\"\", \"three four two three four\", \"four four three three\"]\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, axis=2)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uncount_and_strip(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        for _ in range(3):\n",
    "            voc.add(\"one\")\n",
    "        for _ in range(5):\n",
    "            voc.add(\"two\")\n",
    "        for _ in range(3):\n",
    "            voc.uncount(\"two\")\n",
    "        voc.strip(min_freq=3)\n",
    "        assert \"one\" in voc.string\n",
    "        assert \"two\" not in voc.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uncount_to_death(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        for _ in range(3):\n",
    "            voc.add(\"one\")\n",
    "        for _ in range(5):\n",
    "            voc.add(\"two\")\n",
    "        for _ in range(5):\n",
    "            voc.uncount(\"two\")\n",
    "        assert \"one\" in voc.string\n",
    "        assert \"two\" not in voc.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(klass, num):\n",
    "    tests = [test_add,\n",
    "             test_specials_init,\n",
    "             test_strip_min_freq,\n",
    "             test_strip_by_n,\n",
    "             test_num_identity,\n",
    "             test_num_identity_unk,\n",
    "             test_out_of_vocab_no_unk,\n",
    "             test_sentence_1d,\n",
    "             test_sentence_1d_unk,\n",
    "             test_sentence_2d_axis_0,\n",
    "             test_sentence_2d_axis_1,\n",
    "             test_sentence_3d_axis_2,\n",
    "             test_uncount_and_strip,\n",
    "             test_uncount_to_death]\n",
    "    for test in tests:\n",
    "        print(f\"Running {test.__name__}\")\n",
    "        test(klass, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_data(file=\"rando.txt\"):\n",
    "    # note: inefficient, but who cares?\n",
    "    with open(file, \"r\") as f:\n",
    "        newtxt = []\n",
    "        txt = f.read()\n",
    "        for c in txt:\n",
    "            if c == \".\":\n",
    "                c = \" .\"\n",
    "            newtxt.append(c)\n",
    "    txt = \"\".join(newtxt).lower()\n",
    "    txt = txt.replace(\"\\n\", \"\").split(\" \")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_adding_iterable(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        start = time.time()\n",
    "        voc.add_iterable(txt)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_adding_iterable.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_adding_iterable.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_numericalizing(voc, num, n_trials=10000):\n",
    "    queries = list(range(1000))\n",
    "    start = time.time()\n",
    "    # TODO: Numericalization is allowed to be destructive. This isn't necessarily right\n",
    "    for _ in queries:\n",
    "        _ = num(voc)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_numericalizing.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_numericalizing.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_single_word_random_access(voc, n_trials=1000):\n",
    "    queries = [random.randint(0, len(voc)-1) for _ in range(n_trials)]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.string[query]\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_single_word_random_access.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_single_word_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_1d_word_array_random_access(voc, n_trials=1000):\n",
    "    lens = [random.randint(3, 12) for _ in range(n_trials)]\n",
    "    queries = [[random.randint(0, len(voc)-1) for _ in range(len_)] for len_ in lens]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.string[query]\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_1d_word_array_random_access.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_1d_word_array_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_1d_word_array_structured_random_access(voc, n_trials=10000):\n",
    "    seq_len = 20\n",
    "    queries = [np.random.randint(0, len(voc)-1, (seq_len)) for _ in range(n_trials)]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.sentence(query, axis=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_1d_word_array_structured_random_access.__name__}\"\n",
    "          f\" ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_1d_word_array_structured_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_2d_word_array_random_access(voc, n_trials=1000):\n",
    "    batch = 64\n",
    "    all_lens = [[random.randint(3, 12) for _ in range(batch)] for _ in range(n_trials)]\n",
    "    queries = [\n",
    "        [[random.randint(0, len(voc)-1) for _ in range(l)] for l in lens]\n",
    "        for lens in all_lens]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.string[query]\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_2d_word_array_random_access.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_2d_word_array_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_2d_word_array_structured_random_access(voc, n_trials=10000):\n",
    "    seq_len = 20\n",
    "    batch = 64\n",
    "    queries = [np.random.randint(0, len(voc) - 1, (seq_len, batch)) for _ in range(n_trials)]\n",
    "    stops = [np.random.randint(1, seq_len, (batch,)) for _ in range(n_trials)]\n",
    "    for i in range(n_trials):\n",
    "        queries[i][..., stops[i]] = 0\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.sentence(query, axis=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_2d_word_array_structured_random_access.__name__}\"\n",
    "          f\" ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_2d_word_array_structured_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_3d_word_array_structured_random_access(voc, n_trials=10000):\n",
    "    seq_len = 20\n",
    "    batch = 64\n",
    "    beam_size = 10\n",
    "    queries = [np.random.randint(0, len(voc) - 1, (seq_len, beam_size, batch)) for _ in range(n_trials)]\n",
    "    stops = [np.random.randint(1, seq_len, (beam_size, batch)) for _ in range(n_trials)]\n",
    "    for i in range(n_trials):\n",
    "        queries[i][..., stops[i]] = 0\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.sentence(query, axis=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_3d_word_array_structured_random_access.__name__}\"\n",
    "          f\" ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_3d_word_array_structured_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_uncounting_iterable(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        txt_to_rm = fake_data(\"rando_short.txt\")\n",
    "        start = time.time()\n",
    "        voc.uncount_iterable(txt_to_rm)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_uncounting_iterable.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_uncounting_iterable.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_adding_iterable(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        start = time.time()\n",
    "        voc.add_iterable(txt)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_adding_iterable.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_adding_iterable.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_n_words(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        n_to_keep = int(len(voc) * 0.20)\n",
    "        start = time.time()\n",
    "        voc.strip(n_to_keep=n_to_keep)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_n_words.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_n_words.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_n_words_numericalized(klass, num, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        voc = num(voc)\n",
    "        n_to_keep = int(len(voc) * 0.20)\n",
    "        start = time.time()\n",
    "        voc.strip(n_to_keep=n_to_keep)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_n_words_numericalized.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_n_words_numericalized.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_by_freq(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        start = time.time()\n",
    "        voc.strip(min_freq=3)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_by_freq.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_by_freq.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_by_freq_numericalized(klass, num, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        voc = num(voc)\n",
    "        start = time.time()\n",
    "        voc.strip(min_freq=3)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_by_freq_numericalized.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_by_freq_numericalized.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarks(klass, num, unk):\n",
    "    bench = []\n",
    "    bench.append(benchmark_adding_iterable(klass, unk))\n",
    "    bench.append(benchmark_uncounting_iterable(klass, unk))\n",
    "    bench.append(benchmark_strip_n_words(klass, unk))\n",
    "    bench.append(benchmark_strip_n_words_numericalized(klass, num, unk))\n",
    "    bench.append(benchmark_strip_by_freq(klass, unk))\n",
    "    bench.append(benchmark_strip_by_freq_numericalized(klass, num, unk))\n",
    "    \n",
    "    voc = klass(unk=unk)\n",
    "    # benchmark\n",
    "    txt = fake_data()\n",
    "    voc.add_iterable(txt)\n",
    "    bench.append(benchmark_numericalizing(voc, num))\n",
    "    voc = num(voc)\n",
    "    \n",
    "    bench.append(benchmark_single_word_random_access(voc))\n",
    "    \n",
    "    # 1D word array random access\n",
    "    bench.append(benchmark_1d_word_array_random_access(voc))\n",
    "    \n",
    "    # 1D word array, structured random access\n",
    "    bench.append(benchmark_1d_word_array_structured_random_access(voc))\n",
    "    \n",
    "    # 2D word array, random access\n",
    "    bench.append(benchmark_2d_word_array_random_access(voc))\n",
    "    \n",
    "    # 2D word array, structured random access\n",
    "    bench.append(benchmark_2d_word_array_structured_random_access(voc))\n",
    "    \n",
    "    # 3D word array, structured random access\n",
    "    bench.append(benchmark_3d_word_array_structured_random_access(voc))\n",
    "    return dict(bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benches = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import bisect\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _VocStrInterface:\n",
    "    def __init__(self, s2c):\n",
    "        self.s2c = s2c\n",
    "        \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.s2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, specials=set(), unk=False):\n",
    "        self.unk = unk\n",
    "        self.specials = specials\n",
    "        self.s2c = Counter()\n",
    "        self.specials_w_unk = deepcopy(self.specials)\n",
    "        if self.unk:\n",
    "            self.specials_w_unk.add(self.unk)\n",
    "        for word in self.specials_w_unk:\n",
    "            self.s2c[word] = float(\"inf\")\n",
    "        self.string = _VocStrInterface(self.s2c)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s2c)\n",
    "        \n",
    "    def add(self, word):\n",
    "        if word in self.s2c:\n",
    "            self.s2c[word] += 1\n",
    "        else:\n",
    "            self.s2c[word] = 1\n",
    "            \n",
    "    def add_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.add(word)\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.s2c[word] -= 1\n",
    "        if self.s2c[word] == 0:\n",
    "            del self.s2c[word]\n",
    "    \n",
    "    def uncount_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.uncount(word)\n",
    "                    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "        if n_to_keep < len(self.s2c):\n",
    "            s2c_n = self.s2c.most_common(n_to_keep)\n",
    "        else:\n",
    "            s2c_n = [(s, c) for s, c in self.s2c.items()]\n",
    "\n",
    "        s2n_f = {s: c for s, c in self.s2c.items() if c >= min_freq}\n",
    "        if minimal:\n",
    "            self.s2c = Counter({s: c for s, c in s2c_n if s in s2n_f})\n",
    "        else:\n",
    "            s2c_n = Counter(s2c_n)\n",
    "            s2c_n.update(s_f)\n",
    "            self.s2c = s2c_n\n",
    "        self.string = _VocStrInterface(self.s2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num .str\n",
    "class _NpStrInterface:\n",
    "    def __init__(self, i2s):\n",
    "        self.i2s = i2s\n",
    "    \n",
    "    def __getitem__(self, integer):\n",
    "        try:\n",
    "            return self.i2s[integer]\n",
    "        except:\n",
    "            return [self.i2s[i] for i in integer]\n",
    "    \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NpIntInterface:\n",
    "    def __init__(self, s2i, unk):\n",
    "        self.s2i = s2i\n",
    "        if unk is not False:\n",
    "            self.unk = unk\n",
    "    \n",
    "    def __getitem__(self, string):\n",
    "        if isinstance(string, str):\n",
    "            try:\n",
    "                return self.s2i[string]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    return self.unk\n",
    "                except AttributeError:\n",
    "                    raise KeyError(f\"Couldn't find {string}\")\n",
    "        else:\n",
    "            return [self[s] for s in string]\n",
    "    \n",
    "    def __contains__(self, int_):\n",
    "        return int_ < len(self.s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationNP:\n",
    "    def __init__(self, vocab):\n",
    "        self.specials = vocab.specials\n",
    "        self.unk = vocab.unk\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=object)\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        unk_interface = False if not self.unk else self.s2i[self.unk]\n",
    "        self.integer = _NpIntInterface(self.s2i, unk_interface)\n",
    "        self.specs_as_int = np.asarray(self.integer[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.integer[self.specials_w_unk], dtype=np.int64)\n",
    "        self.permit_unk(False)\n",
    "        self.string = _NpStrInterface(self.i2s)\n",
    "    \n",
    "    def sentence(self, integers, axis=0):\n",
    "        sucks = np.isin(integers, self.spec_ints).cumsum(axis=axis) != 0\n",
    "        strs = self.i2s[integers]\n",
    "        idx = tuple([slice(0, s) if i != axis else slice(1, s) for i, s in enumerate(strs.shape)])\n",
    "        strs[idx] = \" \" + strs[idx]\n",
    "        strs[sucks] = \"\"\n",
    "        strs = strs.sum(axis=axis)\n",
    "        return strs\n",
    "    \n",
    "    def permit_unk(self, val):\n",
    "        if val:\n",
    "            self.spec_ints = self.specs_as_int\n",
    "        else:\n",
    "            self.spec_ints = self.specs_w_unk_as_int\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.integer = _NpIntInterface(self.s2i, unk_idx)\n",
    "        self.string = _NpStrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(Vocab, NumericalizationNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_np = benchmarks(Vocab, NumericalizationNP, unk=False)\n",
    "benches[\"numpy\"] = bench_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CyStrInterface:\n",
    "    def __init__(self, i2s):\n",
    "        self.i2s = i2s\n",
    "    \n",
    "    def __getitem__(self, integer):\n",
    "        try:\n",
    "            return self.i2s[integer]\n",
    "        except:\n",
    "            return [self.i2s[i] for i in integer]\n",
    "    \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CyIntInterface:\n",
    "    def __init__(self, s2i, unk):\n",
    "        self.s2i = s2i\n",
    "        if unk is not False:\n",
    "            self.unk = unk\n",
    "    \n",
    "    def __getitem__(self, string):\n",
    "        if isinstance(string, str):\n",
    "            try:\n",
    "                return self.s2i[string]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    return self.unk\n",
    "                except AttributeError:\n",
    "                    raise KeyError(f\"Couldn't find {string}\")\n",
    "        else:\n",
    "            return [self[s] for s in string]\n",
    "    \n",
    "    def __contains__(self, int_):\n",
    "        return int_ < len(self.s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef str _sentence_1d(\n",
    "        np.ndarray[LONG_t, ndim=1] integers,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    cdef np.ndarray[object, ndim=1] strs = strings[integers]\n",
    "    strs[1:] = \" \" + strs[1:]\n",
    "    strs[np.isin(integers, specs_as_int).cumsum() != 0] = \"\"\n",
    "    cdef str string = strs.sum()\n",
    "    return string\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef np.ndarray _sentence(\n",
    "        np.ndarray integers,\n",
    "        int axis,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    cdef np.ndarray strs = strings[integers]\n",
    "    cdef list idx_l = []\n",
    "    cdef Py_ssize_t shape\n",
    "    cdef int i\n",
    "    for i in range(0, strs.ndim):\n",
    "        shape = strs.shape[i]\n",
    "        if i == axis:\n",
    "            idx_l.append(slice(1, shape))\n",
    "            continue\n",
    "        idx_l.append(slice(0, shape))\n",
    "    cdef tuple idx_t = tuple(idx_l)\n",
    "    strs[idx_t] = \" \" + strs[idx_t]\n",
    "    strs[np.isin(integers, specs_as_int).cumsum(axis=axis) != 0] = \"\"\n",
    "    strs = strs.sum(axis=axis)\n",
    "    return strs\n",
    "\n",
    "def sentence(np.ndarray integers,\n",
    "        int axis,\n",
    "        np.ndarray[LONG_t, ndim=1] specs,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    if integers.ndim == 1:\n",
    "        return _sentence_1d(integers, specs, strings)\n",
    "    else:\n",
    "        return _sentence(integers, axis, specs, strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationCy:\n",
    "    def __init__(self, vocab):\n",
    "        self.specials = vocab.specials\n",
    "        self.unk = vocab.unk\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=object)\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.integer = _CyIntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.specs_as_int = np.asarray(self.integer[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.integer[self.specials_w_unk], dtype=np.int64)\n",
    "        self.permit_unk(False)\n",
    "        self.string = _CyStrInterface(self.i2s)\n",
    "        \n",
    "    def permit_unk(self, val):\n",
    "        if val:\n",
    "            self.specs = self.specs_as_int\n",
    "        else:\n",
    "            self.specs = self.specs_w_unk_as_int\n",
    "    \n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        return sentence(integers, axis, self.specs, self.i2s)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.integer = _CyIntInterface(self.s2i, unk_idx)\n",
    "        self.string = _CyStrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(Vocab, NumericalizationCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_cy = benchmarks(Vocab, NumericalizationCy, unk=False)\n",
    "benches[\"cython\"] = bench_cy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "ctypedef np.float64_t FLOAT_t\n",
    "ctypedef np.uint8_t BOOL_t\n",
    "\n",
    "\n",
    "cdef class _CyVocStrInterface:\n",
    "    cdef object s2c\n",
    "    \n",
    "    def __init__(self, s2c):\n",
    "        self.s2c = s2c\n",
    "        \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.s2c\n",
    "    \n",
    "\n",
    "cdef class CyVocab:\n",
    "    cpdef public bint has_unk\n",
    "    cpdef public str unk\n",
    "    cpdef public set specials\n",
    "    cdef public object s2c\n",
    "    cdef public set _specials_maybe_w_unk\n",
    "    cpdef public _CyVocStrInterface string\n",
    "    \n",
    "    def __init__(self, specials=set(), unk=False):\n",
    "        if unk is False or unk is None:\n",
    "            self.unk = \"\"\n",
    "            self.has_unk = False\n",
    "        else:\n",
    "            self.unk = unk\n",
    "            self.has_unk = True\n",
    "        self.specials = specials\n",
    "        self.s2c = Counter()\n",
    "        self._specials_maybe_w_unk = deepcopy(self.specials)\n",
    "        if self.has_unk:\n",
    "            self._specials_maybe_w_unk.add(self.unk)\n",
    "        for word in self._specials_maybe_w_unk:\n",
    "            self.s2c[word] = float(\"inf\")\n",
    "        self.string = _CyVocStrInterface(self.s2c)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s2c)\n",
    "        \n",
    "    def add(self, word):\n",
    "        if word in self.s2c:\n",
    "            self.s2c[word] += 1\n",
    "        else:\n",
    "            self.s2c[word] = 1\n",
    "            \n",
    "    def add_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.add(word)\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.s2c[word] -= 1\n",
    "        if self.s2c[word] == 0:\n",
    "            del self.s2c[word]\n",
    "    \n",
    "    def uncount_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.uncount(word)\n",
    "                    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        n_to_keep += len(self._specials_maybe_w_unk)\n",
    "        if n_to_keep < len(self.s2c):\n",
    "            s2c_n = self.s2c.most_common(n_to_keep)\n",
    "        else:\n",
    "            s2c_n = [(s, c) for s, c in self.s2c.items()]\n",
    "\n",
    "        s2n_f = {s: c for s, c in self.s2c.items() if c >= min_freq}\n",
    "        if minimal:\n",
    "            self.s2c = Counter({s: c for s, c in s2c_n if s in s2n_f})\n",
    "        else:\n",
    "            s2c_n = Counter(s2c_n)\n",
    "            s2c_n.update(s2n_f)\n",
    "            self.s2c = s2c_n\n",
    "        self.string = _CyVocStrInterface(self.s2c)\n",
    "\n",
    "\n",
    "cdef class _CyStrInterface:\n",
    "    cdef readonly np.ndarray i2s\n",
    "    def __init__(self, i2s):\n",
    "        self.i2s = i2s\n",
    "    \n",
    "    def __getitem__(self, integer):\n",
    "        try:\n",
    "            return self.i2s[integer]\n",
    "        except:\n",
    "            return [self.i2s[i] for i in integer]\n",
    "    \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.i2s\n",
    "\n",
    "\n",
    "cdef class _CyIntInterface:\n",
    "    cdef readonly dict s2i\n",
    "    cdef readonly int unk_i\n",
    "    cdef readonly bint has_unk\n",
    "    def __init__(self, s2i, unk, has_unk):\n",
    "        SENTINEL = -50\n",
    "        self.s2i = s2i\n",
    "        if has_unk:\n",
    "            self.unk_i = self.s2i[unk]\n",
    "        else:\n",
    "            self.unk_i = SENTINEL\n",
    "        self.has_unk = has_unk\n",
    "    \n",
    "    def __getitem__(self, string):\n",
    "        if isinstance(string, str):\n",
    "            try:\n",
    "                return self.s2i[string]\n",
    "            except KeyError:\n",
    "                if self.has_unk:\n",
    "                    return self.unk_i\n",
    "                else:\n",
    "                    raise IndexError(f\"Couldn't find {string}\")\n",
    "        else:\n",
    "            return [self[s] for s in string]\n",
    "    \n",
    "    def __contains__(self, int_):\n",
    "        return int_ < len(self.s2i)\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.    \n",
    "cdef np.ndarray[BOOL_t, ndim=1, cast=True] _isin_cumsum_ge0(np.ndarray[LONG_t, ndim=1] elements, int cutoff):\n",
    "    n_elems = len(elements)\n",
    "    cdef int elem_idx\n",
    "    for elem_idx in range(0, n_elems):\n",
    "        if elements[elem_idx] < cutoff:\n",
    "            return np.concatenate((np.zeros(elem_idx, dtype=bool), np.ones(n_elems - elem_idx, dtype=bool)))\n",
    "    \n",
    "    return np.zeros(n_elems, dtype=bool)\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef str _sentence_1d(\n",
    "        np.ndarray[LONG_t, ndim=1] integers,\n",
    "        int cutoff,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    cdef np.ndarray[object, ndim=1] strs = strings[integers]\n",
    "    strs[1:] = \" \" + strs[1:]\n",
    "    strs[_isin_cumsum_ge0(integers, cutoff)] = \"\"\n",
    "    cdef str string = strs.sum()\n",
    "    return string\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef np.ndarray _sentence(\n",
    "        np.ndarray integers,\n",
    "        int axis,\n",
    "        int cutoff,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    cdef np.ndarray strs = strings[integers]\n",
    "    cdef list idx_l = []\n",
    "    cdef Py_ssize_t shape\n",
    "    cdef int i\n",
    "    for i in range(0, strs.ndim):\n",
    "        shape = strs.shape[i]\n",
    "        if i == axis:\n",
    "            idx_l.append(slice(1, shape))\n",
    "            continue\n",
    "        idx_l.append(slice(0, shape))\n",
    "    cdef tuple idx_t = tuple(idx_l)\n",
    "    strs[idx_t] = \" \" + strs[idx_t]\n",
    "    strs[(integers < cutoff).cumsum(axis=axis) != 0] = \"\"\n",
    "    strs = strs.sum(axis=axis)\n",
    "    return strs\n",
    "\n",
    "    \n",
    "cdef class NumericalizationCy3:\n",
    "    cpdef readonly str unk\n",
    "    cpdef readonly bint has_unk\n",
    "    cpdef readonly set specials\n",
    "    cdef readonly set _specials_maybe_w_unk\n",
    "    cpdef readonly _CyStrInterface string\n",
    "    cpdef readonly _CyIntInterface integer\n",
    "    cdef readonly np.ndarray cts\n",
    "    cdef readonly np.ndarray i2s\n",
    "    cdef readonly dict s2i\n",
    "    cdef readonly int _len_cts\n",
    "    cdef readonly int _n_spec\n",
    "    cdef readonly int _cutoff\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.specials = vocab.specials\n",
    "        self.unk = vocab.unk\n",
    "        self.has_unk = vocab.has_unk\n",
    "        self._specials_maybe_w_unk = vocab._specials_maybe_w_unk\n",
    "        self._n_spec = len(self._specials_maybe_w_unk)\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=object)\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        # move unk to last position so that it gets threshed\n",
    "        if self.has_unk:\n",
    "            unk_at = self.s2i[self.unk]\n",
    "            if unk_at != self._n_spec - 1:\n",
    "                unk_to = self._n_spec - 1\n",
    "                self.s2i[self.unk] = unk_to\n",
    "                was_at_unk_to = self.i2s[unk_to]\n",
    "                self.s2i[was_at_unk_to] = unk_at\n",
    "                self.i2s[unk_at], self.i2s[unk_to] = self.i2s[unk_to], self.i2s[unk_at]\n",
    "                self.cts[unk_at], self.cts[unk_to] = self.cts[unk_to], self.cts[unk_to]\n",
    "                \n",
    "        self.integer = _CyIntInterface(self.s2i, self.unk, self.has_unk)\n",
    "        self.permit_unk(False)\n",
    "        self.string = _CyStrInterface(self.i2s)\n",
    "        self._len_cts = len(self.cts)\n",
    "    \n",
    "    def permit_unk(self, val):\n",
    "        if val:\n",
    "            self._cutoff = self._n_spec - 1\n",
    "        else:\n",
    "            self._cutoff = self._n_spec\n",
    "    \n",
    "    def sentence(self, integers, axis=0):\n",
    "        if integers.ndim == 1:\n",
    "            return _sentence_1d(integers, self._cutoff, self.i2s)\n",
    "        else:\n",
    "            return _sentence(integers, axis, self._cutoff, self.i2s)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len_cts\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = self._len_cts - np.searchsorted(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = self._len_cts\n",
    "            \n",
    "        n_to_keep += self._n_spec\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self._len_cts = len(self.cts)\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.integer = _CyIntInterface(self.s2i, self.unk, self.has_unk)\n",
    "        self.string = _CyStrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_all(CyVocab, NumericalizationCy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bench_cy3 = benchmarks(CyVocab, NumericalizationCy3, unk=False)\n",
    "benches[\"cython3\"] = bench_cy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import bisect\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "ctypedef np.float64_t FLOAT_t\n",
    "ctypedef np.uint8_t BOOL_t\n",
    "\n",
    "\n",
    "cdef class _CyVocStrInterface:\n",
    "    cdef object s2c\n",
    "    \n",
    "    def __init__(self, s2c):\n",
    "        self.s2c = s2c\n",
    "        \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.s2c\n",
    "    \n",
    "\n",
    "cdef class CyVocab:\n",
    "    cpdef public bint has_unk\n",
    "    cpdef public str unk\n",
    "    cpdef public set specials\n",
    "    cdef public object s2c\n",
    "    cdef public set _specials_maybe_w_unk\n",
    "    cpdef public _CyVocStrInterface string\n",
    "    \n",
    "    def __init__(self, specials=set(), unk=False):\n",
    "        if unk is False or unk is None:\n",
    "            self.unk = \"\"\n",
    "            self.has_unk = False\n",
    "        else:\n",
    "            self.unk = unk\n",
    "            self.has_unk = True\n",
    "        self.specials = specials\n",
    "        self.s2c = Counter()\n",
    "        self._specials_maybe_w_unk = deepcopy(self.specials)\n",
    "        if self.has_unk:\n",
    "            self._specials_maybe_w_unk.add(self.unk)\n",
    "        for word in self._specials_maybe_w_unk:\n",
    "            self.s2c[word] = float(\"inf\")\n",
    "        self.string = _CyVocStrInterface(self.s2c)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s2c)\n",
    "        \n",
    "    def add(self, word):\n",
    "        if word in self.s2c:\n",
    "            self.s2c[word] += 1\n",
    "        else:\n",
    "            self.s2c[word] = 1\n",
    "            \n",
    "    def add_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.add(word)\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.s2c[word] -= 1\n",
    "        if self.s2c[word] == 0:\n",
    "            del self.s2c[word]\n",
    "    \n",
    "    def uncount_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.uncount(word)\n",
    "                    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        n_to_keep += len(self._specials_maybe_w_unk)\n",
    "        if n_to_keep < len(self.s2c):\n",
    "            s2c_n = self.s2c.most_common(n_to_keep)\n",
    "        else:\n",
    "            s2c_n = [(s, c) for s, c in self.s2c.items()]\n",
    "\n",
    "        s2n_f = {s: c for s, c in self.s2c.items() if c >= min_freq}\n",
    "        if minimal:\n",
    "            self.s2c = Counter({s: c for s, c in s2c_n if s in s2n_f})\n",
    "        else:\n",
    "            s2c_n = Counter(s2c_n)\n",
    "            s2c_n.update(s2n_f)\n",
    "            self.s2c = s2c_n\n",
    "        self.string = _CyVocStrInterface(self.s2c)\n",
    "\n",
    "\n",
    "cdef class _CyStrInterface:\n",
    "    cdef readonly np.ndarray i2s\n",
    "    def __init__(self, i2s):\n",
    "        self.i2s = i2s\n",
    "    \n",
    "    def __getitem__(self, integer):\n",
    "        try:\n",
    "            return self.i2s[integer]\n",
    "        except:\n",
    "            return [self.i2s[i] for i in integer]\n",
    "    \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.i2s\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.    \n",
    "cdef np.ndarray[BOOL_t, ndim=1, cast=True] _isin_cumsum_ge0(np.ndarray[LONG_t, ndim=1] elements, set test_elements):\n",
    "    cdef int n_elems = len(elements)\n",
    "    cdef int elem_idx\n",
    "    for elem_idx in range(0, n_elems):\n",
    "        if elements[elem_idx] in test_elements:\n",
    "            return np.concatenate((np.zeros(elem_idx, dtype=bool), np.ones(n_elems - elem_idx, dtype=bool)))\n",
    "    \n",
    "    return np.zeros(n_elems, dtype=bool)\n",
    "\n",
    "\n",
    "# @cython.boundscheck(False)  # Deactivate bounds checking\n",
    "# @cython.wraparound(False)   # Deactivate negative indexing.    \n",
    "# cdef np.ndarray[BOOL_t, ndim=1, cast=True] _isin_cumsum_ge0_stepped(\n",
    "#         np.ndarray[LONG_t, ndim=1] elements, set test_elements, int step):\n",
    "#     cdef int n_elems = len(elements)\n",
    "#     cdef np.ndarray[BOOL_t, ndim=1, cast=True] mask = np.zeros(n_elems, dtype=bool)\n",
    "# #     cdef int elem_idx = 0\n",
    "# #     cdef int group = 1\n",
    "#     cdef int i\n",
    "#     for i in range(0, n_elems, step):\n",
    "#         mask[i:i+step] = _isin_cumsum_ge0(elements[i:i+step], test_elements)\n",
    "# #     while elem_idx < n_elems:\n",
    "# #         if elements[elem_idx] in test_elements:\n",
    "# #             mask[elem_idx:(group * step)] = 1\n",
    "# #             elem_idx = group * step\n",
    "# #             group += 1\n",
    "# #             continue\n",
    "# #         elem_idx += 1\n",
    "# #         if elem_idx % step == 0:\n",
    "# #             group += 1\n",
    "#     return mask\n",
    "            \n",
    "\n",
    "# @cython.boundscheck(False)  # Deactivate bounds checking\n",
    "# @cython.wraparound(False)   # Deactivate negative indexing.    \n",
    "# cdef np.ndarray _isin_cumsum_ge0_general(np.ndarray elements, set test_elements, int axis):\n",
    "#     cdef Py_ssize_t stepsize = elements.shape[axis]\n",
    "#     if axis != 0:\n",
    "#         elements = np.swapaxes(elements, axis, 0)\n",
    "#         return _isin_cumsum_ge0_stepped(np.ravel(elements, 'F'), test_elements, stepsize).reshape(\n",
    "#             [elements.shape[i] for i in range(elements.ndim)], order=\"F\").swapaxes(0, axis)\n",
    "#     else:\n",
    "#         return _isin_cumsum_ge0_stepped(np.ravel(elements, 'F'), test_elements, stepsize).reshape(\n",
    "#             [elements.shape[i] for i in range(elements.ndim)], order=\"F\")\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.    \n",
    "cdef np.ndarray _isin_cumsum_ge0_general(np.ndarray elements, np.ndarray test_elements, int axis):\n",
    "    cdef np.ndarray [LONG_t, ndim=1] elements_1 = elements.ravel()\n",
    "    mask = np.zeros(len(elements_1), dtype=bool)\n",
    "    cdef int test_idx\n",
    "    for test_idx in range(0, len(test_elements)):\n",
    "        mask |= (elements_1 == test_elements[test_idx])\n",
    "    return mask.reshape([elements.shape[i] for i in range(elements.ndim)]).cumsum(axis=axis) != 0\n",
    "    \n",
    "    \n",
    "cdef class _CyIntInterface:\n",
    "    cdef readonly dict s2i\n",
    "    cdef readonly int unk_i\n",
    "    cdef readonly bint has_unk\n",
    "    def __init__(self, s2i, unk, has_unk):\n",
    "        SENTINEL = -50\n",
    "        self.s2i = s2i\n",
    "        if has_unk:\n",
    "            self.unk_i = self.s2i[unk]\n",
    "        else:\n",
    "            self.unk_i = SENTINEL\n",
    "        self.has_unk = has_unk\n",
    "    \n",
    "    def __getitem__(self, string):\n",
    "        if isinstance(string, str):\n",
    "            try:\n",
    "                return self.s2i[string]\n",
    "            except KeyError:\n",
    "                if self.has_unk:\n",
    "                    return self.unk_i\n",
    "                else:\n",
    "                    raise IndexError(f\"Couldn't find {string}\")\n",
    "        else:\n",
    "            return [self[s] for s in string]\n",
    "    \n",
    "    def __contains__(self, int_):\n",
    "        return int_ < len(self.s2i)\n",
    "    \n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef str _sentence_1d(\n",
    "        np.ndarray[LONG_t, ndim=1] integers,\n",
    "        # np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        set specs_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    # cdef np.ndarray sucks = _isin(integers, specs_as_int).cumsum() != 0\n",
    "    # cdef np.ndarray sucks = _isin_cumsum_ge0(integers, specs_as_int)\n",
    "    cdef np.ndarray[object, ndim=1] strs = strings[integers]\n",
    "    strs[1:] = \" \" + strs[1:]\n",
    "    strs[_isin_cumsum_ge0(integers, specs_as_int)] = \"\"\n",
    "    cdef str string = strs.sum()\n",
    "    return string\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef np.ndarray _sentence(\n",
    "        np.ndarray integers,\n",
    "        int axis,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        # set specs_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "#     cdef np.ndarray sucks = np.isin(integers, specs_as_int).cumsum(axis=axis) != 0\n",
    "#     cdef np.ndarray sucks = \n",
    "#     cdef np.ndarray sucks = _isin_cumsum_ge0_general(integers, specs_as_int, axis=axis)\n",
    "    cdef np.ndarray strs = strings[integers]\n",
    "    cdef list idx_l = []\n",
    "    cdef Py_ssize_t shape\n",
    "    cdef int i\n",
    "    for i in range(0, strs.ndim):\n",
    "        shape = strs.shape[i]\n",
    "        if i == axis:\n",
    "            idx_l.append(slice(1, shape))\n",
    "            continue\n",
    "        idx_l.append(slice(0, shape))\n",
    "    cdef tuple idx_t = tuple(idx_l)\n",
    "    strs[idx_t] = \" \" + strs[idx_t]\n",
    "    strs[_isin_cumsum_ge0_general(integers, specs_as_int, axis)] = \"\"\n",
    "    strs = strs.sum(axis=axis)\n",
    "    return strs\n",
    "\n",
    "    \n",
    "cdef class NumericalizationCy2:\n",
    "    cpdef readonly str unk\n",
    "    cpdef readonly bint has_unk\n",
    "    cpdef readonly set specials\n",
    "    cdef readonly set _specials_maybe_w_unk\n",
    "    cdef readonly np.ndarray _specs_as_int\n",
    "    cdef readonly np.ndarray _specs_maybe_w_unk_as_int\n",
    "    cdef readonly set _chosen_specs_as_int_set\n",
    "    cpdef readonly _CyStrInterface string\n",
    "    cpdef readonly _CyIntInterface integer\n",
    "    cdef readonly np.ndarray cts\n",
    "    cdef readonly np.ndarray i2s\n",
    "    cdef readonly dict s2i\n",
    "    cdef readonly int _len_cts\n",
    "    cdef readonly np.ndarray _chosen_specs_as_int\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.specials = vocab.specials\n",
    "        self.unk = vocab.unk\n",
    "        self.has_unk = vocab.has_unk\n",
    "        self._specials_maybe_w_unk = vocab._specials_maybe_w_unk\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=object)\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.integer = _CyIntInterface(self.s2i, self.unk, self.has_unk)\n",
    "        self._specs_as_int = np.asarray(self.integer[self.specials], dtype=np.int64)\n",
    "        self._specs_maybe_w_unk_as_int = np.asarray(self.integer[self._specials_maybe_w_unk], dtype=np.int64)\n",
    "        self.permit_unk(False)\n",
    "        self.string = _CyStrInterface(self.i2s)\n",
    "        self._len_cts = len(self.cts)\n",
    "    \n",
    "    def permit_unk(self, val):\n",
    "        if val:\n",
    "            self._chosen_specs_as_int = self._specs_as_int\n",
    "        else:\n",
    "            self._chosen_specs_as_int = self._specs_maybe_w_unk_as_int\n",
    "        self._chosen_specs_as_int_set = set(self._chosen_specs_as_int)\n",
    "    \n",
    "    def sentence(self, integers, axis=0):\n",
    "        if integers.ndim == 1:\n",
    "            # return _sentence_1d(integers, self._chosen_specs_as_int, self.i2s)\n",
    "            return _sentence_1d(integers, self._chosen_specs_as_int_set, self.i2s)\n",
    "        else:\n",
    "            return _sentence(integers, axis, self._chosen_specs_as_int, self.i2s)\n",
    "            # return _sentence(integers, axis, self._chosen_specs_as_int_set, self.i2s)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len_cts\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = self._len_cts - np.searchsorted(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = self._len_cts\n",
    "            \n",
    "        n_to_keep += len(self._specials_maybe_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self._len_cts = len(self.cts)\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.integer = _CyIntInterface(self.s2i, self.unk, self.has_unk)\n",
    "        self.string = _CyStrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_all(CyVocab, NumericalizationCy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bench_cy2 = benchmarks(CyVocab, NumericalizationCy2, unk=False)\n",
    "benches[\"cython2\"] = bench_cy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(benches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(idx):\n",
    "    results.loc[idx].plot.bar(title=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_3d_word_array_structured_random_access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All is essentially equal in the 3D lookup world. Note that these benchmarks use a cutoff for \"added realism,\" while the old feasibility did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_2d_word_array_structured_random_access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like either full-Cython implementations win, but we're definitely splitting hairs. Again, cutoff guaranteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_1d_word_array_structured_random_access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's an interesting one. There's actually no guaranteed cutoff in this case. Looks like cython's have better worst-case characteristics (could be wrong - the np.concat is maybe horribly heavy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_1d_word_array_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_single_word_random_access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the most interesting... I don't have an explanation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_2d_word_array_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langtools]",
   "language": "python",
   "name": "conda-env-langtools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
