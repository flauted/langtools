{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Analysis of Vocab+Num Design Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        for numericalize in [True, False]:\n",
    "            voc = klass(unk=unk)\n",
    "            voc.add(\"hello\")\n",
    "            if numericalize:\n",
    "                voc = num(voc)\n",
    "            assert \"hello\" in voc.str\n",
    "            if unk is not False:\n",
    "                assert unk in voc.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_specials_init(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        for numericalize in [True, False]:\n",
    "            specs = {\"<pad>\", \"<bos>\", \"<eos>\"}\n",
    "            voc = klass(specials=specs, unk=unk)\n",
    "            if numericalize:\n",
    "                voc = num(voc)\n",
    "            for spec in specs:\n",
    "                assert spec in voc.str\n",
    "            if unk is not False:\n",
    "                assert unk in voc.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strip_min_freq(klass, num):\n",
    "    for numericalize in [True, False]:\n",
    "        for unk in [\"UNK\", False]:\n",
    "            all_words = {\"one\", \"two\", \"three\", \"three_again\", \"four\"}\n",
    "            for freq, *words in [\n",
    "                    (0, \"one\", \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (1, \"one\", \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (2, \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (3, \"three\", \"three_again\", \"four\"),\n",
    "                    (4, \"four\")]:\n",
    "                voc = klass(unk=unk)\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three\")\n",
    "                for _ in range(2):\n",
    "                    voc.add(\"two\")\n",
    "                for _ in range(1):\n",
    "                    voc.add(\"one\")\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three_again\")\n",
    "                for _ in range(4):\n",
    "                    voc.add(\"four\")\n",
    "                \n",
    "                if numericalize:\n",
    "                    voc = num(voc)\n",
    "\n",
    "                voc.strip(min_freq=freq)\n",
    "                if unk:\n",
    "                    assert unk in voc.str\n",
    "                for word in words:\n",
    "                    assert word in voc.str\n",
    "                for word in all_words:\n",
    "                    if word not in words:\n",
    "                        try:\n",
    "                            assert word not in voc.str\n",
    "                        except:\n",
    "                            print(f\"case numericalize={numericalize} \"\n",
    "                                  f\"unk={unk} freq={freq} word={word} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strip_by_n(klass, num):\n",
    "    for numericalize in [True, False]:\n",
    "        for unk in [\"UNK\", False]:\n",
    "            all_words = {\"one\", \"two\", \"three\", \"three_again\", \"four\"}\n",
    "            for n_words, *words in [\n",
    "                    (5, \"one\", \"two\", \"three\", \"three_again\", \"four\"),\n",
    "                    (4, \"four\", \"three\", \"three_again\", \"two\"),\n",
    "                    (3, \"four\", \"three\", \"three_again\"),\n",
    "                    (1, \"four\")]:\n",
    "                voc = klass(unk=unk)\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three\")\n",
    "                for _ in range(2):\n",
    "                    voc.add(\"two\")\n",
    "                for _ in range(1):\n",
    "                    voc.add(\"one\")\n",
    "                for _ in range(3):\n",
    "                    voc.add(\"three_again\")\n",
    "                for _ in range(4):\n",
    "                    voc.add(\"four\")\n",
    "\n",
    "                if numericalize:\n",
    "                    voc = num(voc)\n",
    "\n",
    "                voc.strip(n_to_keep=n_words)\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        assert word in voc.str\n",
    "                    except:\n",
    "                        print(f\"case IN numericalize={numericalize} \"\n",
    "                              f\"unk={unk} n_words={n_words} word={word} failed\")\n",
    "                for word in all_words:\n",
    "                    if word not in words:\n",
    "                        try:\n",
    "                            assert word not in voc.str\n",
    "                        except:\n",
    "                            print(\n",
    "                                f\"case NOT_IN numericalize={numericalize} \"\n",
    "                                f\"unk={unk} n_words={n_words} word={word} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_num_identity(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        voc.add(\"hello\")\n",
    "        voc = num(voc)\n",
    "        assert \"hello\" == voc.str[voc.int[\"hello\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_num_identity_unk(klass, num):\n",
    "    voc = klass(unk=\"UNK\")\n",
    "    voc = num(voc)\n",
    "    assert \"UNK\" == voc.str[voc.int[\"jambalaya\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out_of_vocab_no_unk(klass, num):\n",
    "    voc = klass(unk=False)\n",
    "    voc = num(voc)\n",
    "    try:\n",
    "        voc.str[voc.int[\"jambalaya\"]]\n",
    "    except:\n",
    "        pass  # lol, PASS!\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_1d(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [2, 3, 4, 1, 2]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three four\"\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_1d_unk(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [2, 3, 4, 0, 0]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three four UNK UNK\"\n",
    "    ])\n",
    "    voc = klass(unk=\"UNK\")\n",
    "    for _ in range(6):\n",
    "        voc.add(\"one\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, permit_unk=True)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_2d_axis_0(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [[2, 3, 4, 1, 2],\n",
    "         [3, 2, 4, 2, 1],\n",
    "         [2, 3, 0, 0, 0]]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three two\", \"three two three\", \"four four\", \"\", \"two\"\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, axis=0)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_2d_axis_1(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [[2, 3, 4, 1, 2],\n",
    "         [3, 2, 4, 2, 1],\n",
    "         [2, 3, 0, 0, 0]]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        \"two three four\", \"three two four two\", \"two three\"\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, axis=1)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence_3d_axis_2(klass, num):\n",
    "    fake_data = np.asarray(\n",
    "        [[[2, 3, 4, 1, 2],\n",
    "          [3, 2, 4, 2, 1],\n",
    "          [2, 3, 0, 0, 0]],\n",
    "         [[1, 0, 0, 0, 0],\n",
    "          [3, 4, 2, 3, 4],\n",
    "          [4, 4, 3, 3, 0]]]\n",
    "    )\n",
    "    expected = np.asarray([\n",
    "        [\"two three four\", \"three two four two\", \"two three\"],\n",
    "        [\"\", \"three four two three four\", \"four four three three\"]\n",
    "    ])\n",
    "    voc = klass(specials={\"one\"}, unk=\"UNK\")\n",
    "    for _ in range(5):\n",
    "        voc.add(\"two\")\n",
    "    for _ in range(4):\n",
    "        voc.add(\"three\")\n",
    "    for _ in range(3):\n",
    "        voc.add(\"four\")\n",
    "    voc = num(voc)\n",
    "    s = voc.sentence(fake_data, axis=2)\n",
    "    assert (expected == s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uncount_and_strip(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        for _ in range(3):\n",
    "            voc.add(\"one\")\n",
    "        for _ in range(5):\n",
    "            voc.add(\"two\")\n",
    "        for _ in range(3):\n",
    "            voc.uncount(\"two\")\n",
    "        voc.strip(min_freq=3)\n",
    "        assert \"one\" in voc.str\n",
    "        assert \"two\" not in voc.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uncount_and_strip_numericalized(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        for _ in range(3):\n",
    "            voc.add(\"one\")\n",
    "        for _ in range(5):\n",
    "            voc.add(\"two\")\n",
    "        for _ in range(4):\n",
    "            voc.uncount(\"two\")\n",
    "        voc = num(voc)\n",
    "        voc.strip(min_freq=3)\n",
    "        voc.renumericalize()\n",
    "        \n",
    "        assert \"one\" in voc.str\n",
    "        assert \"two\" not in voc.str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uncount_to_death(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        for _ in range(3):\n",
    "            voc.add(\"one\")\n",
    "        for _ in range(5):\n",
    "            voc.add(\"two\")\n",
    "        for _ in range(5):\n",
    "            voc.uncount(\"two\")\n",
    "        assert \"one\" in voc.str\n",
    "        assert \"two\" not in voc.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uncount_to_death_numericalized(klass, num):\n",
    "    for unk in [False, \"UNK\"]:\n",
    "        voc = klass(unk=unk)\n",
    "        for _ in range(3):\n",
    "            voc.add(\"one\")\n",
    "        for _ in range(5):\n",
    "            voc.add(\"two\")\n",
    "        voc = num(voc)\n",
    "        for _ in range(5):\n",
    "            voc.uncount(\"two\")\n",
    "        voc.renumericalize()\n",
    "        assert \"one\" in voc.str\n",
    "        assert \"two\" not in voc.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(klass, num):\n",
    "    tests = [test_add,\n",
    "             test_specials_init,\n",
    "             test_strip_min_freq,\n",
    "             test_strip_by_n,\n",
    "             test_num_identity,\n",
    "             test_num_identity_unk,\n",
    "             test_out_of_vocab_no_unk,\n",
    "             test_sentence_1d,\n",
    "             test_sentence_1d_unk,\n",
    "             test_sentence_2d_axis_0,\n",
    "             test_sentence_2d_axis_1,\n",
    "             test_sentence_3d_axis_2,\n",
    "             test_uncount_and_strip,\n",
    "             test_uncount_and_strip_numericalized,\n",
    "             test_uncount_to_death,\n",
    "             test_uncount_to_death_numericalized]\n",
    "    for test in tests:\n",
    "        print(f\"Running {test.__name__}\")\n",
    "        test(klass, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_data(file=\"rando.txt\"):\n",
    "    # note: inefficient, but who cares?\n",
    "    with open(file, \"r\") as f:\n",
    "        newtxt = []\n",
    "        txt = f.read()\n",
    "        for c in txt:\n",
    "            if c == \".\":\n",
    "                c = \" .\"\n",
    "            newtxt.append(c)\n",
    "    txt = \"\".join(newtxt).lower()\n",
    "    txt = txt.replace(\"\\n\", \"\").split(\" \")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_adding_iterable(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        start = time.time()\n",
    "        voc.add_iterable(txt)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_adding_iterable.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_adding_iterable.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_adding_iterable_numericalized(klass, num, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data(\"rando_short.txt\")\n",
    "        voc.add_iterable(txt)\n",
    "        voc = num(voc)\n",
    "        new_txt = fake_data(\"rando.txt\")\n",
    "        start = time.time()\n",
    "        voc.add_iterable(new_txt)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_adding_iterable_numericalized.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_adding_iterable_numericalized.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_numericalizing(voc, num, n_trials=1000):\n",
    "    queries = list(range(1000))\n",
    "    start = time.time()\n",
    "    # TODO: Numericalization is allowed to be destructive. This isn't necessarily right\n",
    "    for _ in queries:\n",
    "        _ = num(voc)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_numericalizing.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_numericalizing.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_single_word_random_access(voc, n_trials=1000):\n",
    "    queries = [random.randint(0, len(voc)-1) for _ in range(n_trials)]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.str[query]\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_single_word_random_access.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_single_word_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_1d_word_array_random_access(voc, n_trials=1000):\n",
    "    lens = [random.randint(3, 12) for _ in range(n_trials)]\n",
    "    queries = [[random.randint(0, len(voc)-1) for _ in range(len_)] for len_ in lens]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.str[query]\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_1d_word_array_random_access.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_1d_word_array_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_1d_word_array_structured_random_access(voc, n_trials=1000):\n",
    "    seq_len = 20\n",
    "    queries = [np.random.randint(0, len(voc)-1, (seq_len)) for _ in range(n_trials)]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.sentence(query, axis=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_1d_word_array_structured_random_access.__name__}\"\n",
    "          f\" ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_1d_word_array_structured_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_2d_word_array_random_access(voc, n_trials=1000):\n",
    "    batch = 64\n",
    "    all_lens = [[random.randint(3, 12) for _ in range(batch)] for _ in range(n_trials)]\n",
    "    queries = [\n",
    "        [[random.randint(0, len(voc)-1) for _ in range(l)] for l in lens]\n",
    "        for lens in all_lens]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.str[query]\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_2d_word_array_random_access.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_2d_word_array_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_2d_word_array_structured_random_access(voc, n_trials=1000):\n",
    "    seq_len = 20\n",
    "    batch = 64\n",
    "    queries = [np.random.randint(0, len(voc) - 1, (seq_len, batch)) for _ in range(n_trials)]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.sentence(query, axis=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_2d_word_array_structured_random_access.__name__}\"\n",
    "          f\" ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_2d_word_array_structured_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_3d_word_array_structured_random_access(voc, n_trials=1000):\n",
    "    seq_len = 20\n",
    "    batch = 64\n",
    "    beam_size = 10\n",
    "    queries = [np.random.randint(0, len(voc) - 1, (seq_len, beam_size, batch)) for _ in range(n_trials)]\n",
    "    start = time.time()\n",
    "    for query in queries:\n",
    "        _ = voc.sentence(query, axis=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_3d_word_array_structured_random_access.__name__}\"\n",
    "          f\" ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_3d_word_array_structured_random_access.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_uncounting_iterable(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        txt_to_rm = fake_data(\"rando_short.txt\")\n",
    "        start = time.time()\n",
    "        voc.uncount_iterable(txt_to_rm)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_uncounting_iterable.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_uncounting_iterable.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_uncounting_iterable__and_renumericalizing_numericalized(klass, num, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        voc = num(voc)\n",
    "        txt_to_rm = fake_data(\"rando_short.txt\")\n",
    "        start = time.time()\n",
    "        voc.uncount_iterable(txt_to_rm)\n",
    "        voc.renumericalize()\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_uncounting_iterable__and_renumericalizing_numericalized.__name__} \"\n",
    "          f\"({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_uncounting_iterable__and_renumericalizing_numericalized.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_adding_iterable(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        start = time.time()\n",
    "        voc.add_iterable(txt)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_adding_iterable.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_adding_iterable.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_n_words(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        n_to_keep = int(len(voc) * 0.20)\n",
    "        start = time.time()\n",
    "        voc.strip(n_to_keep=n_to_keep)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_n_words.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_n_words.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_n_words_numericalized(klass, num, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        voc = num(voc)\n",
    "        n_to_keep = int(len(voc) * 0.20)\n",
    "        start = time.time()\n",
    "        voc.strip(n_to_keep=n_to_keep)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_n_words_numericalized.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_n_words_numericalized.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_by_freq(klass, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        start = time.time()\n",
    "        voc.strip(min_freq=3)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_by_freq.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_by_freq.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_strip_by_freq_numericalized(klass, num, unk, n_trials=10):\n",
    "    elapsed = 0\n",
    "    for _ in range(n_trials):\n",
    "        voc = klass(unk=unk)\n",
    "        txt = fake_data()\n",
    "        voc.add_iterable(txt)\n",
    "        voc = num(voc)\n",
    "        start = time.time()\n",
    "        voc.strip(min_freq=3)\n",
    "        end = time.time()\n",
    "        elapsed += end - start\n",
    "    avg = elapsed / n_trials\n",
    "    print(f\"{benchmark_strip_by_freq_numericalized.__name__} ({n_trials}): {avg} s, avg\")\n",
    "    return (benchmark_strip_by_freq_numericalized.__name__, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarks(klass, num, unk):\n",
    "    bench = []\n",
    "    bench.append(benchmark_adding_iterable(klass, unk))\n",
    "    bench.append(benchmark_adding_iterable_numericalized(klass, num, unk))\n",
    "    bench.append(benchmark_uncounting_iterable(klass, unk))\n",
    "    bench.append(benchmark_uncounting_iterable__and_renumericalizing_numericalized(\n",
    "        klass, num, unk))\n",
    "    bench.append(benchmark_strip_n_words(klass, unk))\n",
    "    bench.append(benchmark_strip_n_words_numericalized(klass, num, unk))\n",
    "    bench.append(benchmark_strip_by_freq(klass, unk))\n",
    "    bench.append(benchmark_strip_by_freq_numericalized(klass, num, unk))\n",
    "    \n",
    "    voc = klass(unk=unk)\n",
    "    # benchmark\n",
    "    txt = fake_data()\n",
    "    voc.add_iterable(txt)\n",
    "    bench.append(benchmark_numericalizing(voc, num))\n",
    "    voc = num(voc)\n",
    "    \n",
    "    bench.append(benchmark_single_word_random_access(voc))\n",
    "    \n",
    "    # 1D word array random access\n",
    "    bench.append(benchmark_1d_word_array_random_access(voc))\n",
    "    \n",
    "    # 1D word array, structured random access\n",
    "    bench.append(benchmark_1d_word_array_structured_random_access(voc))\n",
    "    \n",
    "    # 2D word array, random access\n",
    "    bench.append(benchmark_2d_word_array_random_access(voc))\n",
    "    \n",
    "    # 2D word array, structured random access\n",
    "    bench.append(benchmark_2d_word_array_structured_random_access(voc))\n",
    "    \n",
    "    # 3D word array, structured random access\n",
    "    bench.append(benchmark_3d_word_array_structured_random_access(voc))\n",
    "    return dict(bench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set .str\n",
    "class _DumbStrInterface:\n",
    "    def __init__(self, s2c):\n",
    "        self.s2c = s2c\n",
    "        \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.s2c\n",
    "    \n",
    "# num .str\n",
    "class _StrInterface:\n",
    "    def __init__(self, i2s):\n",
    "        self.i2s = i2s\n",
    "    \n",
    "    def __getitem__(self, integer):\n",
    "        # This could actually be tuned to the implementation\n",
    "        try:\n",
    "            return self.i2s[integer]\n",
    "        except:\n",
    "            return [self[i] for i in integer]\n",
    "    \n",
    "    def __contains__(self, str_):\n",
    "        return str_ in self.i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num .int (set doesn't have one obviously)\n",
    "class _IntInterface:\n",
    "    def __init__(self, s2i, unk):\n",
    "        self.s2i = s2i\n",
    "        if unk is not False:\n",
    "            self.unk = unk\n",
    "    \n",
    "    def __getitem__(self, string):\n",
    "        if isinstance(string, str):\n",
    "            try:\n",
    "                return self.s2i[string]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    return self.unk\n",
    "                except AttributeError:\n",
    "                    raise KeyError(f\"Couldn't find {string}\")\n",
    "        else:\n",
    "            return [self[s] for s in string]\n",
    "    \n",
    "    def __contains__(self, int_):\n",
    "        return int_ < len(self.s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabPromise:\n",
    "    def __init__(self, specials, unk):\n",
    "        assert isinstance(specials, set)\n",
    "        assert unk is False or isinstance(unk, str)\n",
    "        self.unk = unk\n",
    "        self.specials = specials\n",
    "        self.str = self.specials\n",
    "        \n",
    "    def __len__(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def add(self, word):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def add_iterable(self, words):\n",
    "        # This could maybe be cythonized for added efficiency\n",
    "        for word in words:\n",
    "            self.add(word)\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def uncount_iterable(self, words):\n",
    "        for word in words:\n",
    "            self.uncount(word)\n",
    "            \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "benches = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1 - Pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabOne(VocabPromise):\n",
    "    def __init__(self, specials=set(), unk=False):\n",
    "        super().__init__(specials=specials, unk=unk)\n",
    "        self.s2c = Counter()\n",
    "        self.specials_w_unk = deepcopy(self.specials)\n",
    "        if self.unk:\n",
    "            self.specials_w_unk.add(self.unk)\n",
    "        for word in self.specials_w_unk:\n",
    "            self.s2c[word] = float(\"inf\")\n",
    "        self.str = _DumbStrInterface(self.s2c)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.s2c)\n",
    "        \n",
    "    def add(self, word):\n",
    "        if word in self.s2c:\n",
    "            self.s2c[word] += 1\n",
    "        else:\n",
    "            self.s2c[word] = 1\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.s2c[word] -= 1\n",
    "        if self.s2c[word] == 0:\n",
    "            del self.s2c[word]\n",
    "                    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "        if n_to_keep < len(self.s2c):\n",
    "            s2c_n = self.s2c.most_common(n_to_keep)\n",
    "        else:\n",
    "            s2c_n = [(s, c) for s, c in self.s2c.items()]\n",
    "\n",
    "        s2n_f = {s: c for s, c in self.s2c.items() if c >= min_freq}\n",
    "        if minimal:\n",
    "            self.s2c = Counter({s: c for s, c in s2c_n if s in s2n_f})\n",
    "        else:\n",
    "            s2c_n = Counter(s2c_n)\n",
    "            s2c_n.update(s_f)\n",
    "            self.s2c = s2c_n\n",
    "        self.str = _DumbStrInterface(self.s2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationOne(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "\n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        shape = list(integers.shape)\n",
    "        axes = list(range(len(shape)))\n",
    "        if permit_unk:\n",
    "            spec_ints = self.specs_as_int\n",
    "        else:\n",
    "            spec_ints = self.specs_w_unk_as_int\n",
    "        del axes[axis]\n",
    "        axes = [axis] + axes\n",
    "        del shape[axis]\n",
    "        strs = np.full(shape, \"\", dtype=np.object)\n",
    "        if axis != 0:\n",
    "            integers = np.transpose(integers, axes)\n",
    "        frame = integers[0]\n",
    "        alive = np.isin(frame, spec_ints, invert=True)\n",
    "        additions = self.str[frame[alive]]\n",
    "        strs[alive] = additions\n",
    "        for frame in integers[1:]:\n",
    "            alive_now = np.isin(frame, spec_ints, invert=True)\n",
    "            alive = alive & alive_now\n",
    "            additions = self.str[frame[alive]]\n",
    "            strs[alive] += \" \"\n",
    "            strs[alive] += additions\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test_add\n",
      "Running test_specials_init\n",
      "Running test_strip_min_freq\n",
      "Running test_strip_by_n\n",
      "Running test_num_identity\n",
      "Running test_num_identity_unk\n",
      "Running test_out_of_vocab_no_unk\n",
      "Running test_sentence_1d\n",
      "Running test_sentence_1d_unk\n",
      "Running test_sentence_2d_axis_0\n",
      "Running test_sentence_2d_axis_1\n",
      "Running test_sentence_3d_axis_2\n",
      "Running test_uncount_and_strip\n",
      "Running test_uncount_and_strip_numericalized\n",
      "Running test_uncount_to_death\n",
      "Running test_uncount_to_death_numericalized\n"
     ]
    }
   ],
   "source": [
    "test_all(VocabOne, NumericalizationOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark_adding_iterable (10): 0.00508427619934082 s, avg\n",
      "benchmark_adding_iterable_numericalized (10): 0.07520151138305664 s, avg\n",
      "benchmark_uncounting_iterable (10): 0.00015342235565185547 s, avg\n",
      "benchmark_uncounting_iterable__and_renumericalizing_numericalized (10): 0.0001361370086669922 s, avg\n",
      "benchmark_strip_n_words (10): 0.00047442913055419924 s, avg\n",
      "benchmark_strip_n_words_numericalized (10): 3.120899200439453e-05 s, avg\n",
      "benchmark_strip_by_freq (10): 0.00029382705688476565 s, avg\n",
      "benchmark_strip_by_freq_numericalized (10): 9.319782257080078e-05 s, avg\n",
      "benchmark_numericalizing (1000): 0.0003815882205963135 s, avg\n",
      "benchmark_single_word_random_access (1000): 1.2731552124023438e-07 s, avg\n",
      "benchmark_1d_word_array_random_access (1000): 1.48773193359375e-06 s, avg\n",
      "benchmark_1d_word_array_structured_random_access (1000): 0.00035651874542236327 s, avg\n",
      "benchmark_2d_word_array_random_access (1000): 9.727787971496581e-05 s, avg\n",
      "benchmark_2d_word_array_structured_random_access (1000): 0.0009130918979644775 s, avg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-93b14893a29c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, integer)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1d4028bb36c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbench_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVocabOne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumericalizationOne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbenches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pure python\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbench_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-cb5addf51126>\u001b[0m in \u001b[0;36mbenchmarks\u001b[0;34m(klass, num, unk)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# 3D word array, structured random access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_3d_word_array_structured_random_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbench\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-11871e251c88>\u001b[0m in \u001b[0;36mbenchmark_3d_word_array_structured_random_access\u001b[0;34m(voc, n_trials)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-de3f91746922>\u001b[0m in \u001b[0;36msentence\u001b[0;34m(self, integers, axis, permit_unk)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0malive_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_ints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0malive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malive\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0malive_now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0madditions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mstrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mstrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0madditions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-93b14893a29c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, integer)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-93b14893a29c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bench_1 = benchmarks(VocabOne, NumericalizationOne, unk=False)\n",
    "benches[\"pure python\"] = bench_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2 - Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationTwo(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=np.unicode)\n",
    "        del vocab\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "    \n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        shape = list(integers.shape)\n",
    "        axes = list(range(len(shape)))\n",
    "        if permit_unk:\n",
    "            spec_ints = self.specs_as_int\n",
    "        else:\n",
    "            spec_ints = self.specs_w_unk_as_int\n",
    "        del axes[axis]\n",
    "        axes = [axis] + axes\n",
    "        del shape[axis]\n",
    "        strs = np.full(shape, \"\", dtype=np.object)\n",
    "        if axis != 0:\n",
    "            integers = np.transpose(integers, axes)\n",
    "        frame = integers[0]\n",
    "        alive = np.isin(frame, spec_ints, invert=True)\n",
    "        additions = self.str[frame[alive]]\n",
    "        strs[alive] = additions\n",
    "        for frame in integers[1:]:\n",
    "            alive_now = np.isin(frame, spec_ints, invert=True)\n",
    "            alive = alive & alive_now\n",
    "            additions = self.str[frame[alive]]\n",
    "            strs[alive] += \" \"\n",
    "            strs[alive] += additions\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts = np.append(self.cts, 1)\n",
    "            self.i2s = np.append(self.i2s, word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = self.cts <= 0\n",
    "        alives = ~kills\n",
    "        dead_words = self.i2s[kills]\n",
    "        self.cts = self.cts[alives]\n",
    "        self.i2s = self.i2s[alives]\n",
    "        for word in dead_words:\n",
    "            del self.s2i[word]\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_2 = benchmarks(VocabOne, NumericalizationTwo, unk=False)\n",
    "benches[\"numpy\"] = bench_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3 - Python + Naive Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "def sentence(np.ndarray[LONG_t, ndim=2] integers,\n",
    "             bint permit_unk,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "             strings):\n",
    "    cdef Py_ssize_t sent_len = integers.shape[0]\n",
    "    cdef Py_ssize_t n_sents = integers.shape[1]\n",
    "    \n",
    "    cdef np.ndarray[LONG_t, ndim=1] spec_ints\n",
    "    if permit_unk:\n",
    "        spec_ints = specs_as_int\n",
    "    else:\n",
    "        spec_ints = specs_w_unk_as_int\n",
    "    \n",
    "    cdef np.ndarray joined_strings = np.full((n_sents,), \"\", dtype=object)\n",
    "    cdef LONG_t integer\n",
    "\n",
    "    for i_sent in range(0, n_sents):\n",
    "        integer = integers[0, i_sent]\n",
    "        if integer in spec_ints:\n",
    "            continue\n",
    "        joined_strings[i_sent] += strings[integer]\n",
    "        for i_word in range(1, sent_len):\n",
    "            integer = integers[i_word, i_sent]\n",
    "            if integer in spec_ints:\n",
    "                break\n",
    "            joined_strings[i_sent] += \" \" + strings[integer]\n",
    "\n",
    "    return joined_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationThree(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.spec_ints = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.spec_w_unk_ints = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "    \n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        shape = list(integers.shape)\n",
    "        if axis != 0:\n",
    "            axes = list(range(len(shape)))\n",
    "            axes.insert(0, axes.pop(axis))\n",
    "            integers = np.transpose(integers, axes)\n",
    "\n",
    "        seq_size = shape.pop(axis)\n",
    "\n",
    "        integers = integers.reshape(seq_size, -1)\n",
    "        strs = sentence(integers, permit_unk, self.spec_ints, self.spec_w_unk_ints, self.i2s)\n",
    "        strs = strs.reshape(shape)\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationThree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_3 = benchmarks(VocabOne, NumericalizationThree, unk=False)\n",
    "benches[\"python + naive cython\"] = bench_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4 - Python + Naive Cython Dispatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "def sentence_1d(np.ndarray[LONG_t, ndim=1] integers,\n",
    "             bint permit_unk,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "             strings):\n",
    "    cdef Py_ssize_t sent_len = integers.shape[0]\n",
    "    \n",
    "    cdef np.ndarray[LONG_t, ndim=1] spec_ints\n",
    "    if permit_unk:\n",
    "        spec_ints = specs_as_int\n",
    "    else:\n",
    "        spec_ints = specs_w_unk_as_int\n",
    "    \n",
    "    cdef LONG_t integer\n",
    "    substrings = []\n",
    "    for integer in integers:\n",
    "        if integer in spec_ints:\n",
    "            break\n",
    "        substrings.append(strings[integer])\n",
    "\n",
    "    return np.array([\" \".join(substrings)], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "def sentence_2d(np.ndarray[LONG_t, ndim=2] integers,\n",
    "             bint permit_unk,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "             strings):\n",
    "    cdef Py_ssize_t sent_len = integers.shape[0]\n",
    "    cdef Py_ssize_t n_sents = integers.shape[1]\n",
    "    \n",
    "    cdef np.ndarray[LONG_t, ndim=1] spec_ints\n",
    "    if permit_unk:\n",
    "        spec_ints = specs_as_int\n",
    "    else:\n",
    "        spec_ints = specs_w_unk_as_int\n",
    "    \n",
    "    cdef np.ndarray joined_strings = np.full((n_sents,), \"\", dtype=object)\n",
    "    cdef LONG_t integer\n",
    "\n",
    "    cdef int i_sent, i_word\n",
    "    for i_sent in range(0, n_sents):\n",
    "        for i_word in range(0, sent_len):\n",
    "            integer = integers[i_word, i_sent]\n",
    "            if integer in spec_ints:\n",
    "                break\n",
    "            if i_word > 0:\n",
    "                joined_strings[i_sent] += \" \"\n",
    "            joined_strings[i_sent] += strings[integer]\n",
    "        \n",
    "    return joined_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "def sentence_3d(np.ndarray[LONG_t, ndim=3] integers,\n",
    "             bint permit_unk,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "             strings):\n",
    "    cdef Py_ssize_t sent_len = integers.shape[0]\n",
    "    cdef Py_ssize_t n_sents = integers.shape[1]\n",
    "    cdef Py_ssize_t n_sents_2 = integers.shape[2]\n",
    "    \n",
    "    cdef np.ndarray[LONG_t, ndim=1] spec_ints\n",
    "    if permit_unk:\n",
    "        spec_ints = specs_as_int\n",
    "    else:\n",
    "        spec_ints = specs_w_unk_as_int\n",
    "    \n",
    "    cdef np.ndarray joined_strings = np.empty((n_sents, n_sents_2), dtype=object)\n",
    "    cdef LONG_t integer\n",
    "\n",
    "    for i_sent in range(0, n_sents):\n",
    "        for i_sent_2 in range(0, n_sents_2):\n",
    "            substrings = []\n",
    "            for i_word in range(0, sent_len):\n",
    "                integer = integers[i_word, i_sent, i_sent_2]\n",
    "                if integer in spec_ints:\n",
    "                    break\n",
    "                substrings.append(strings[integer])\n",
    "            joined_strings[i_sent, i_sent_2] = \" \".join(substrings)\n",
    "        \n",
    "    return joined_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationFour(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.spec_ints = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.spec_w_unk_ints = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "\n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        dim = integers.ndim\n",
    "        if axis != 0:\n",
    "            axes = list(range(dim))\n",
    "            axes.insert(0, axes.pop(axis))\n",
    "            shape = list(integers.shape)\n",
    "            integers = np.transpose(integers, axes)\n",
    "        if dim == 1:\n",
    "            strs = sentence_1d(integers, permit_unk, self.spec_ints, self.spec_w_unk_ints, self.i2s)\n",
    "        elif dim == 2:\n",
    "            strs = sentence_2d(integers, permit_unk, self.spec_ints, self.spec_w_unk_ints, self.i2s)\n",
    "        elif dim == 3:\n",
    "            strs = sentence_3d(integers, permit_unk, self.spec_ints, self.spec_w_unk_ints, self.i2s)\n",
    "        else:\n",
    "            assert False\n",
    "        if axis != 0:\n",
    "            del shape[axis]\n",
    "            strs = strs.reshape(shape)\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationFour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_4 = benchmarks(VocabOne, NumericalizationFour, unk=False)\n",
    "benches[\"python + naive cython dispatched\"] = bench_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Five - Python + Vectorized Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "def sentence(np.ndarray integers,\n",
    "             bint permit_unk,\n",
    "             int axis,\n",
    "             np.ndarray[LONG_t, ndim=1] desired_shape,\n",
    "             np.ndarray[LONG_t, ndim=1] transpose_axes,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "             np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "             np.ndarray[object, ndim=1] strings):\n",
    "    if not permit_unk:\n",
    "        specs_as_int = specs_w_unk_as_int\n",
    "    cdef np.ndarray strs = np.full(desired_shape, \"\", dtype=object)\n",
    "    if axis != 0:\n",
    "        integers = np.transpose(integers, transpose_axes)\n",
    "    \n",
    "    frame = integers[0, ...]\n",
    "    alive = np.isin(frame, specs_as_int, invert=True)\n",
    "    strs[alive] = strings[frame[alive]]\n",
    "    for frame in integers[1:]:\n",
    "        alive = alive & np.isin(frame, specs_as_int, invert=True)\n",
    "        strs[alive] += \" \"\n",
    "        strs[alive] += strings[frame[alive]]\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationFive(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "\n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        return sentence(\n",
    "            integers,\n",
    "            permit_unk,\n",
    "            axis,\n",
    "            np.asarray([s for i, s in enumerate(integers.shape) if i != axis], dtype=np.int64),\n",
    "            np.asarray([axis] + [i for i in range(integers.ndim) if i != axis], dtype=np.int64),\n",
    "            self.specs_as_int,\n",
    "            self.specs_w_unk_as_int,\n",
    "            np.asarray(self.i2s, dtype=np.object))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationFive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_5 = benchmarks(VocabOne, NumericalizationFive, unk=False)\n",
    "benches[\"python + vectorized cython\"] = bench_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 6 - Python + Dispatched Vectorized Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "def sentence_general(\n",
    "        np.ndarray integers,\n",
    "        bint permit_unk,\n",
    "        int axis,\n",
    "        np.ndarray[LONG_t, ndim=1] desired_shape,\n",
    "        np.ndarray[LONG_t, ndim=1] transpose_axes,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    if not permit_unk:\n",
    "        specs_as_int = specs_w_unk_as_int\n",
    "    cdef np.ndarray strs = np.full(desired_shape, \"\", dtype=object)\n",
    "    if axis != 0:\n",
    "        integers = np.transpose(integers, transpose_axes)\n",
    "    \n",
    "    cdef np.ndarray frame = integers[0, ...]\n",
    "    cdef np.ndarray alive = np.isin(frame, specs_as_int, invert=True)\n",
    "    strs[alive] = strings[frame[alive]]\n",
    "    for frame in integers[1:]:\n",
    "        alive = alive & np.isin(frame, specs_as_int, invert=True)\n",
    "        strs[alive] += \" \"\n",
    "        strs[alive] += strings[frame[alive]]\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "def sentence_1d(\n",
    "        np.ndarray[LONG_t, ndim=1] integers,\n",
    "        bint permit_unk,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    if not permit_unk:\n",
    "        specs_as_int = specs_w_unk_as_int\n",
    "    cdef Py_ssize_t size = integers.shape[0]\n",
    "    cdef str result = \"\"\n",
    "    \n",
    "    cdef LONG_t frame = integers[0]\n",
    "    if np.isin(frame, specs_as_int, invert=True):\n",
    "        result += strings[frame]\n",
    "    else:\n",
    "        return np.ndarray([result], dtype=object)\n",
    "    for frame in integers[1:]:\n",
    "        if np.isin(frame, specs_as_int, invert=True):\n",
    "            result += \" \"\n",
    "            result += strings[frame]\n",
    "        else:\n",
    "            break\n",
    "    return np.asarray([result], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationSix(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "\n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        if integers.ndim == 1:\n",
    "            return sentence_1d(\n",
    "                integers,\n",
    "                permit_unk,\n",
    "                self.specs_as_int,\n",
    "                self.specs_w_unk_as_int,\n",
    "                np.asarray(self.i2s, dtype=np.object))\n",
    "        else:\n",
    "            return sentence_general(\n",
    "                integers,\n",
    "                permit_unk,\n",
    "                axis,\n",
    "                np.asarray([s for i, s in enumerate(integers.shape) if i != axis], dtype=np.int64),\n",
    "                np.asarray([axis] + [i for i in range(integers.ndim) if i != axis], dtype=np.int64),\n",
    "                self.specs_as_int,\n",
    "                self.specs_w_unk_as_int,\n",
    "                np.asarray(self.i2s, dtype=np.object))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationSix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bench_6 = benchmarks(VocabOne, NumericalizationSix, unk=False)\n",
    "benches[\"python + dispatched vectorized cython\"] = bench_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 7 -  NumPython Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationSeven(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "\n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        strings = np.asarray(self.i2s, dtype=object)\n",
    "        if permit_unk:\n",
    "            spec_ints = self.specs_as_int\n",
    "        else:\n",
    "            spec_ints = self.specs_w_unk_as_int\n",
    "#         benchmark_1d_word_array_structured_random_access (1000): 3.898239135742187e-05 s, avg\n",
    "#         benchmark_2d_word_array_structured_random_access (1000): 0.0001302328109741211 s, avg\n",
    "#         benchmark_3d_word_array_structured_random_access (1000): 0.0009071543216705322 s, avg\n",
    "#         sucks = np.isin(integers, spec_ints).cumsum(axis=axis) > 0\n",
    "# benchmark_1d_word_array_structured_random_access (1000): 3.8427352905273435e-05 s, avg\n",
    "# benchmark_2d_word_array_structured_random_access (1000): 0.00013177490234375 s, avg\n",
    "# benchmark_3d_word_array_structured_random_access (1000): 0.0009074432849884033 s, avg\n",
    "#         sucks = np.isin(integers, spec_ints).cumsum(axis=axis).astype(bool)\n",
    "# benchmark_1d_word_array_structured_random_access (1000): 4.0471553802490234e-05 s, avg\n",
    "# benchmark_2d_word_array_structured_random_access (1000): 0.00012927746772766114 s, avg\n",
    "# benchmark_3d_word_array_structured_random_access (1000): 0.0008900551795959472 s, avg\n",
    "        # since it's a toss-up, this one's the most conceptually accurate.\n",
    "        sucks = np.isin(integers, spec_ints).cumsum(axis=axis) != 0\n",
    "        strs = strings[integers]\n",
    "        idx = tuple([slice(0, s) if i != axis else slice(1, s) for i, s in enumerate(strs.shape)])\n",
    "#         idx[axis] = slice(1, n)\n",
    "        # idx = tuple(idx)\n",
    "        strs[idx] = \" \" + strs[idx]\n",
    "        strs[sucks] = \"\"\n",
    "        strs = strs.sum(axis=axis)\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationSeven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_7 = benchmarks(VocabOne, NumericalizationSeven, unk=False)\n",
    "benches[\"numpython improved\"] = bench_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 7.5 - Numpy Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationSevenPointFive(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=object)\n",
    "        del vocab\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "    \n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        if permit_unk:\n",
    "            spec_ints = self.specs_as_int\n",
    "        else:\n",
    "            spec_ints = self.specs_w_unk_as_int\n",
    "        sucks = np.isin(integers, spec_ints).cumsum(axis=axis) != 0\n",
    "        strs = self.i2s[integers]\n",
    "        idx = tuple([slice(0, s) if i != axis else slice(1, s) for i, s in enumerate(strs.shape)])\n",
    "        strs[idx] = \" \" + strs[idx]\n",
    "        strs[sucks] = \"\"\n",
    "        strs = strs.sum(axis=axis)\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts = np.append(self.cts, 1)\n",
    "            self.i2s = np.append(self.i2s, word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = self.cts <= 0\n",
    "        alives = ~kills\n",
    "        dead_words = self.i2s[kills]\n",
    "        self.cts = self.cts[alives]\n",
    "        self.i2s = self.i2s[alives]\n",
    "        for word in dead_words:\n",
    "            del self.s2i[word]\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationSevenPointFive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_7_5 = benchmarks(VocabOne, NumericalizationSevenPointFive, unk=False)\n",
    "benches[\"numpy improved\"] = bench_7_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 7.75 - NumCython Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "ctypedef np.int64_t LONG_t\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef str _sentence_1d(\n",
    "        np.ndarray[LONG_t, ndim=1] integers,\n",
    "        bint permit_unk,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    if not permit_unk:\n",
    "        specs_as_int = specs_w_unk_as_int\n",
    "    cdef np.ndarray sucks = np.isin(integers, specs_as_int).cumsum() != 0\n",
    "    cdef np.ndarray[object, ndim=1] strs = strings[integers]\n",
    "    strs[1:] = \" \" + strs[1:]\n",
    "    strs[sucks] = \"\"\n",
    "    cdef str string = strs.sum()\n",
    "    return string\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "cdef np.ndarray _sentence(\n",
    "        np.ndarray integers,\n",
    "        int axis,\n",
    "        bint permit_unk,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    if not permit_unk:\n",
    "        specs_as_int = specs_w_unk_as_int\n",
    "    cdef np.ndarray sucks = np.isin(integers, specs_as_int).cumsum(axis=axis) != 0\n",
    "    cdef np.ndarray strs = strings[integers]\n",
    "    cdef list idx_l = []\n",
    "    cdef Py_ssize_t shape\n",
    "    cdef int i\n",
    "    for i in range(0, strs.ndim):\n",
    "        shape = strs.shape[i]\n",
    "        if i == axis:\n",
    "            idx_l.append(slice(1, shape))\n",
    "            continue\n",
    "        idx_l.append(slice(0, shape))\n",
    "    cdef tuple idx_t = tuple(idx_l)\n",
    "    strs[idx_t] = \" \" + strs[idx_t]\n",
    "    strs[sucks] = \"\"\n",
    "    strs = strs.sum(axis=axis)\n",
    "    return strs\n",
    "\n",
    "def sentence(np.ndarray integers,\n",
    "        int axis,\n",
    "        bint permit_unk,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_as_int,\n",
    "        np.ndarray[LONG_t, ndim=1] specs_w_unk_as_int,\n",
    "        np.ndarray[object, ndim=1] strings):\n",
    "    if integers.ndim == 1:\n",
    "        return _sentence_1d(integers, permit_unk, specs_as_int, specs_w_unk_as_int, strings)\n",
    "    else:\n",
    "        return _sentence(integers, axis, permit_unk, specs_as_int, specs_w_unk_as_int, strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationSevenPointSevenFive(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = np.asarray(list(vocab.s2c.values()), dtype=np.float64)\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = np.asarray(list(vocab.s2c.keys()), dtype=object)\n",
    "        del vocab\n",
    "        self.cts = unordered_cts[idxs_desc]\n",
    "        self.i2s = unordered_strs[idxs_desc]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "    \n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        return sentence(integers, axis, permit_unk, self.specs_as_int, self.specs_w_unk_as_int, self.i2s)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts = np.append(self.cts, 1)\n",
    "            self.i2s = np.append(self.i2s, word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = self.cts <= 0\n",
    "        alives = ~kills\n",
    "        dead_words = self.i2s[kills]\n",
    "        self.cts = self.cts[alives]\n",
    "        self.i2s = self.i2s[alives]\n",
    "        for word in dead_words:\n",
    "            del self.s2i[word]\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationSevenPointSevenFive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_7_75 = benchmarks(VocabOne, NumericalizationSevenPointSevenFive, unk=False)\n",
    "benches[\"numcython improved\"] = bench_7_75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 8 - NumPython medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizationEight(VocabPromise):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__(specials=vocab.specials, unk=vocab.unk)\n",
    "        self.specials_w_unk = vocab.specials_w_unk\n",
    "        unordered_cts = list(vocab.s2c.values())\n",
    "        idxs_desc = np.argsort(unordered_cts)[::-1]\n",
    "        unordered_strs = list(vocab.s2c.keys())\n",
    "        del vocab\n",
    "        self.cts = [unordered_cts[idx] for idx in idxs_desc]\n",
    "        self.i2s = [unordered_strs[idx] for idx in idxs_desc]\n",
    "        self.dummy = len(self.i2s)\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        self.int = _IntInterface(self.s2i, False if not self.unk else self.s2i[self.unk])\n",
    "        self.str = _StrInterface(self.i2s)\n",
    "        self.specs_as_int = np.asarray(self.int[self.specials], dtype=np.int64)\n",
    "        self.specs_w_unk_as_int = np.asarray(self.int[self.specials_w_unk], dtype=np.int64)\n",
    "\n",
    "    def sentence(self, integers, axis=0, permit_unk=False):\n",
    "        strings = np.asarray(self.i2s + [\"\"], dtype=object)\n",
    "        if permit_unk:\n",
    "            spec_ints = self.specs_as_int\n",
    "        else:\n",
    "            spec_ints = self.specs_w_unk_as_int\n",
    "        sucks = np.isin(integers, spec_ints).cumsum(axis=axis) != 0\n",
    "        integers[sucks] = self.dummy\n",
    "        strs = strings[integers]\n",
    "        idx = tuple([slice(0, s) if i != axis else 0 for i, s in enumerate(strs.shape)])\n",
    "        needs_space = ~sucks\n",
    "        needs_space[idx] = False\n",
    "        strs[needs_space] = \" \" + strs[needs_space]\n",
    "        strs = strs.sum(axis=axis)\n",
    "        return strs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cts)\n",
    "\n",
    "    def add(self, word):\n",
    "        if word in self.str:\n",
    "            self.cts[self.int[word]] += 1\n",
    "        else:\n",
    "            idx = len(self.i2s)\n",
    "            self.cts.append(1)\n",
    "            self.i2s.append(word)\n",
    "            self.s2i[word] = idx\n",
    "    \n",
    "    def uncount(self, word):\n",
    "        self.cts[self.s2i[word]] -= 1\n",
    "\n",
    "    def renumericalize(self):\n",
    "        kills = [i for i, c in enumerate(self.cts) if c <= 0]\n",
    "        for kill in kills:\n",
    "            word = self.i2s[kill]\n",
    "            del self.s2i[word]\n",
    "            del self.i2s[kill]\n",
    "            del self.cts[kill]\n",
    "    \n",
    "    def strip(self, n_to_keep=float(\"inf\"), min_freq=0, minimal=True):\n",
    "        # TODO: This is actually only valid if you haven't been adding...\n",
    "        if min_freq > 0:\n",
    "            n_freq_enough = len(self.cts) - bisect.bisect_left(self.cts[::-1], min_freq)\n",
    "        else:\n",
    "            n_freq_enough = len(self.cts)\n",
    "            \n",
    "        n_to_keep += len(self.specials_w_unk)\n",
    "\n",
    "        if minimal:\n",
    "            n_to_keep = min(n_freq_enough, n_to_keep)\n",
    "        else:\n",
    "            n_to_keep = max(n_freq_enough, n_to_keep)\n",
    "        if n_to_keep >= len(self.cts):\n",
    "            return\n",
    "        self.cts = self.cts[:n_to_keep]\n",
    "        self.i2s = self.i2s[:n_to_keep]\n",
    "        self.s2i = {s: i for i, s in enumerate(self.i2s)}\n",
    "        if self.unk:\n",
    "            unk_idx = self.s2i[self.unk]\n",
    "        else:\n",
    "            unk_idx = False\n",
    "        self.int = _IntInterface(self.s2i, unk_idx)\n",
    "        self.str = _StrInterface(self.i2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_all(VocabOne, NumericalizationEight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_8 = benchmarks(VocabOne, NumericalizationEight, unk=False)\n",
    "benches[\"numpython medium\"] = bench_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "* I think all these implementations are slightly flawed (semantically - I think you could devise tests that break them, particularly by stripping words added to the Numericalization)\n",
    "* The interfaces aren't implementation-optimized. In particular, the numpy versions don't need a try-except on the indexing.\n",
    "* Some of the minor differences can't be statistically meaningful because I'm not benchmarking enough examples.\n",
    "* Doing the entire thing in Cython has advantages. Doing add and add_iterable in Cython would probably speed up the construction time considerably.\n",
    "* Still haven't tested a C++ Vector based implementation.\n",
    "* In all honesty, having an editable numericalization is probably a mistake. If memory isn't a big issue (when talking about 1e6 words) then keeping both numericalization and vectorization around is best solution, and renumericalizing as necessary (an argument for requiring fast numericalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Specs\n",
    "* \"Structured random access\" of 3D arrays is the most important use-case, followed by 2D.\n",
    "* String->int is not necessarily important, but good.\n",
    "* Construction should be relatively fast, but access is significantly more important.\n",
    "* Stripping by frequency should not be terribly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = pd.DataFrame(benches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benchmarks.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(idx):\n",
    "    benchmarks.loc[idx].plot.bar(title=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_1d_word_array_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_2d_word_array_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_1d_word_array_structured_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_2d_word_array_structured_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_3d_word_array_structured_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\"benchmark_single_word_random_access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langtools]",
   "language": "python",
   "name": "conda-env-langtools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
